# def_mod_path = "yolov9-tvm.model"
# def_param_path = "yolov9-tvm.param"
# def_label_yaml = "dataset.yaml"
# def_conf_thres = 0.4
# def_iou_thres = 0.4
# def_dev_type = "llvm"  # 0=tvm.cpu(), 1=tvm.cuda(), other=tvm.opencl()
# def_data_type = "float32"
# def_width = 640 
# def_height = 640
#def_input_location = "./images" # number=camera, url, or file_path
# def_input_location = "./images/horses.jpg" # number=camera, url, or file_path
# def_input_location = "./images/sample_image.jpeg" # number=camera, url, or file_path
# def_input_location = "./images/480.mp4" # number=camera, url, or file_path
# def_output_location = "./result" # 0=screen, 1=text, url,or folder_path
# def_output_location = 0  # 0=screen, 1=text, url,or folder_path

# from user's selection
# input_location can be  a number = camera device id(defualt 0) or
#                        url = stream input or
#                        directory or
#                        file name
# output_location can be number 0 = screen drawing or
#                        number 1 = text output or
#                        url = stream output or
#                        directory 


""" copyright notice
This module is for testing code for neural network model.
"""
###############################################################
# import onnx
import cv2
import numpy as np
import time
import yaml
import os
import sys
import myutil
import tvm
from tvm.runtime import vm as _vm
###
from tvm.relay import transform
import matplotlib.pyplot as plt


def xywh2xyxy(x):
    y = np.copy(x)
    y[:, 0] = x[:, 0] - x[:, 2] / 2
    y[:, 1] = x[:, 1] - x[:, 3] / 2
    y[:, 2] = x[:, 0] + x[:, 2] / 2
    y[:, 3] = x[:, 1] + x[:, 3] / 2
    return y




#############################################
# Class definition for TVM run module  
#############################################
class TVMRun():
    def __init__(self,  
            dev_type=def_dev_type, lyaml=def_label_yaml, 
            input_location=def_input_location, confthr=def_conf_thres, 
            iouthr=def_iou_thres, output_location=def_output_location):
        """
        TVM Runtime class definition
        Args:
            lyaml : yaml file for label info. 
            input_location : [0-9]: camera, URL, file name, or folder name 
            confthr : confidence threshhold 
            iouthr : IOU threshhole
            output_location : 0=screen, 1=text output, or folder name 
        """
        self.dev_type = dev_type
        self.data_type = def_data_type
        self.inputs = []
        self.outputs = []
        self.allocations = []
        self.classes = None
        self.label_yaml =  lyaml
        self.img_folder = input_location 
        self.conf_thres = confthr 
        self.iou_thres = iouthr 
        self.output_location = output_location 
        self.video = 0
        self.vid_writer = 0
        self.vid_path = ""
        self.text_out = False
        self.view_img = False
        self.save_img = False
        self.stream_out = False
        self.width = def_width
        self.height = def_height
        self.dtype = "float32"
        if self.output_location == 0:
            self.view_img = True
        elif self.output_location == 1:
            self.text_out = True
        elif "://" in self.output_location:
            self.stream_out = True
        else:
            self.save_img = True
        with open(self.label_yaml) as f:
            classes = yaml.safe_load(f)
            self.classes = classes['names']
        return

    def load_model(self):
        """
        To load tensorrt model 

        Args:
            none
        Returns: 
            none
        """
        '''
        self.onnx_model = onnx.load(self.lib_path)
        input_name = "images"
        self.shape_dict = {input_name: [1, 3, self.width, self.height]}
        mod, params = tvm.relay.frontend.from_onnx(self.onnx_model, self.shape_dict)
        with open("aaa.mod", "w") as fo:
            fo.write(tvm.ir.save_json(mod))
        with open("aaa.param", "wb") as fo:
            fo.write(tvm.runtime.save_param_dict(params))
        '''
        print("Loading TVM engine module !!!")
        with open(def_mod_path, "r") as fi:
            mod = tvm.ir.load_json(fi.read())
        with open(def_param_path, "rb") as fi:
            params = tvm.runtime.load_param_dict(fi.read())
        with tvm.transform.PassContext(opt_level=1):
            self.executor = tvm.relay.build_module.create_executor(
            "graph", mod, tvm.cpu(0), self.dev_type, params
            ).evaluate()


    def nms(self, boxes, scores, nms_thr):
        x1 = np.array([])
        y1 = np.array([])
        x2 = np.array([])
        y2 = np.array([])
        sz = scores.size
        for i in range(sz):
            x1 = np.append(x1, boxes[i][0])
            y1 = np.append(y1, boxes[i][1])
            x2 = np.append(x2, boxes[i][2])
            y2 = np.append(y2, boxes[i][3])

        areas = (x2 - x1 + 1) * (y2 - y1 + 1)
        order = scores.argsort()[::-1]
        keep = []
        while (order.size) > 0:
            i = order[0]
            keep.append(i)
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])
            w = np.maximum(0.0, xx2 - xx1 + 1)
            h = np.maximum(0.0, yy2 - yy1 + 1)
            inter = w * h
            ovr = inter / (areas[i] + areas[order[1:]] - inter)
            inds = np.where(ovr <= nms_thr)[0]
            order = order[inds + 1]
        return keep


    def multiclass_nms(self, boxes, scores, cls_ids, nms_thr=0.45, score_thr=0.1):
        tch = []
        for i in range(len(self.classes)):
            x =  [i, [], []]
            tch.append(x)
        j = 0
        for item in cls_ids:
            tch[item][1].append(boxes[j])  # boxes
            tch[item][2].append(scores[j])  # confidence
            j = j + 1

        final_dets = []
        num_classes = len(self.classes) 
        for cls_ind in range(num_classes):
            if len(tch[cls_ind][2]) == 0:
                continue
            valid_scores = np.array(tch[cls_ind][2])  
            valid_boxes = np.array(tch[cls_ind][1])  
            keep = self.nms(valid_boxes, valid_scores, nms_thr)
            if len(keep) > 0:
                cls_inds = np.ones((len(keep), 1)) * cls_ind
                dets = np.concatenate(
                    [valid_boxes[keep], valid_scores[keep, None], cls_inds], 1
                )
                final_dets.append(dets)
        if len(final_dets) == 0:
            return None

        return np.concatenate(final_dets, 0)


    
    def rainbow_fill(self, size=50):  
        cmap = plt.get_cmap('jet')
        color_list = []
        for n in range(size):
            color = cmap(n/size)
            color_list.append(color[:3])  
        return np.array(color_list)


    def preprocess(self, image):
        swap = (2, 0, 1)
        padded_img = np.ones((self.width, self.height, 3)) * 114.0
        img = np.array(image)
        r = min(self.width / img.shape[0], self.height / img.shape[1])
        resized_img = cv2.resize(
            img,
            (int(img.shape[1] * r), int(img.shape[0] * r)),
            interpolation=cv2.INTER_LINEAR,
        ).astype(np.float32)
        padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img
        padded_img = padded_img[:, :, ::-1]
        padded_img /= 255.0
        padded_img = padded_img.transpose(swap)
        padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)
        return padded_img, r
    
    def do_camera_infer(self, dev=0, target_folder=""):
        """
        To inference from camera input  

        Args:
            dev : camera device number (defualt: 0)
        Returns: 
            none
        """
        save_path = myutil.get_fullpath(self.save_folder, filename)
        self.video = cv2.VideoCapture(dev)
        self.video.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.video.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        print("Press q to quit")
        while (self.video.isOpened()):
            flag, img = self.video.read()
            if flag == False:
                break
            # preprocessing
            x, ratio = self.preprocess(img)
            #run
            tvm_output = self.executor(tvm.nd.array(x.astype(self.dtype)))
            tvm_output = tvm_output[-1]
            # .numpy()
            tvm_output = tvm_output.numpy()
            tvm_output = np.squeeze(tvm_output).T
            # interpreter ret value
            scores = np.max(tvm_output[:,4:], axis=1)
            predictions = tvm_output[scores > def_conf_thres, :]
            scores = scores[scores > def_conf_thres]
            class_ids = np.argmax(predictions[:, 4:], axis=1)
            boxes = predictions[:, :4]

            input_shape = np.array([self.width, self.height, self.width, self.height])
            boxes = np.divide(boxes, input_shape, dtype=np.float32)
            boxes *= np.array([self.width, self.height, self.width, self.height])
            boxes = boxes.astype(np.int32)

            #predictions = np.reshape(data, (1, tsize, -1))
            # predictions = predictions.tolist()
            # 1, 84, 8400
            # scores = predictions[:, 4:tsize]
            #
            boxes_xyxy = xywh2xyxy(boxes)
            for i in range(len(boxes_xyxy)):
                boxes_xyxy[i, 0] = int(boxes_xyxy[i, 0] / ratio)
                boxes_xyxy[i, 1] = int(boxes_xyxy[i, 1] / ratio)
                boxes_xyxy[i, 2] = int(boxes_xyxy[i, 2] / ratio)
                boxes_xyxy[i, 3] = int(boxes_xyxy[i, 3] / ratio)

            dets = self.multiclass_nms(boxes_xyxy, scores, class_ids, nms_thr=def_iou_thres, score_thr=def_conf_thres)
            if (type(dets) == np.ndarray):
                final_boxes, final_scores, final_cls_inds = dets[:, :4], dets[:, 4], dets[:, 5]
                result = [final_boxes, final_scores, final_cls_inds]
                self.postprocess(img, result, save_path, still_image=True)
                if cv2.waitKey(1) == ord('q'):
                    break
        self.video.release()
        if isinstance(self.vid_writer, cv2.VideoWriter):
            self.vid_writer.release()  
        cv2.destroyAllWindows()
        return

    def do_url_infer(self, url, target_folder=""):
        """
        To inference from streamin data  

        Args:
            url : url for streaming input 
        Returns: 
            none
        """
        save_path = myutil.get_fullpath(target_folder, filename)
        self.video = cv2.VideoCapture(url)
        print("Press q to quit")
        while (self.video.isOpened()):
            flag, img = self.video.read()
            if flag == False:
                break
            # preprocessing
            x, ratio = self.preprocess(img)
            #run
            tvm_output = self.executor(tvm.nd.array(x.astype(self.dtype)))
            tvm_output = tvm_output[-1]
            # .numpy()
            tvm_output = tvm_output.numpy()
            tvm_output = np.squeeze(tvm_output).T
            # interpreter ret value
            scores = np.max(tvm_output[:,4:], axis=1)
            predictions = tvm_output[scores > def_conf_thres, :]
            scores = scores[scores > def_conf_thres]
            class_ids = np.argmax(predictions[:, 4:], axis=1)
            boxes = predictions[:, :4]

            input_shape = np.array([self.width, self.height, self.width, self.height])
            boxes = np.divide(boxes, input_shape, dtype=np.float32)
            boxes *= np.array([self.width, self.height, self.width, self.height])
            boxes = boxes.astype(np.int32)

            #predictions = np.reshape(data, (1, tsize, -1))
            # predictions = predictions.tolist()
            # 1, 84, 8400
            # scores = predictions[:, 4:tsize]
            #
            boxes_xyxy = xywh2xyxy(boxes)
            for i in range(len(boxes_xyxy)):
                boxes_xyxy[i, 0] = int(boxes_xyxy[i, 0] / ratio)
                boxes_xyxy[i, 1] = int(boxes_xyxy[i, 1] / ratio)
                boxes_xyxy[i, 2] = int(boxes_xyxy[i, 2] / ratio)
                boxes_xyxy[i, 3] = int(boxes_xyxy[i, 3] / ratio)

            dets = self.multiclass_nms(boxes_xyxy, scores, class_ids, nms_thr=def_iou_thres, score_thr=def_conf_thres)
            if (type(dets) == np.ndarray):
                final_boxes, final_scores, final_cls_inds = dets[:, :4], dets[:, 4], dets[:, 5]
                result = [final_boxes, final_scores, final_cls_inds]
                self.postprocess(img, result, save_path, still_image=True)
                if cv2.waitKey(1) == ord('q'):
                    break
        self.video.release()
        if isinstance(self.vid_writer, cv2.VideoWriter):
            self.vid_writer.release()  
        cv2.destroyAllWindows()
        return

    def do_video_infer(self, filename, target_folder=""):
        """
        To inference from video file  

        Args:
            filename : video file name  
        Returns: 
            none
        """
        save_path = myutil.get_fullpath(target_folder, filename)
        if filename == "":
            i_file = self.img_folder
        else:
            i_file = filename
        self.video = cv2.VideoCapture(i_file)
        print("Press q to quit")
        while (self.video.isOpened()):
            flag, img = self.video.read()
            if flag == False:
                break
            # preprocessing
            x, ratio = self.preprocess(img)
            #run
            tvm_output = self.executor(tvm.nd.array(x.astype(self.dtype)))
            tvm_output = tvm_output[-1]
            # .numpy()
            tvm_output = tvm_output.numpy()
            tvm_output = np.squeeze(tvm_output).T
            # interpreter ret value
            scores = np.max(tvm_output[:,4:], axis=1)
            predictions = tvm_output[scores > def_conf_thres, :]
            scores = scores[scores > def_conf_thres]
            class_ids = np.argmax(predictions[:, 4:], axis=1)
            boxes = predictions[:, :4]

            input_shape = np.array([self.width, self.height, self.width, self.height])
            boxes = np.divide(boxes, input_shape, dtype=np.float32)
            boxes *= np.array([self.width, self.height, self.width, self.height])
            boxes = boxes.astype(np.int32)

            #predictions = np.reshape(data, (1, tsize, -1))
            # predictions = predictions.tolist()
            # 1, 84, 8400
            # scores = predictions[:, 4:tsize]
            #
            boxes_xyxy = xywh2xyxy(boxes)
            for i in range(len(boxes_xyxy)):
                boxes_xyxy[i, 0] = int(boxes_xyxy[i, 0] / ratio)
                boxes_xyxy[i, 1] = int(boxes_xyxy[i, 1] / ratio)
                boxes_xyxy[i, 2] = int(boxes_xyxy[i, 2] / ratio)
                boxes_xyxy[i, 3] = int(boxes_xyxy[i, 3] / ratio)

            dets = self.multiclass_nms(boxes_xyxy, scores, class_ids, nms_thr=def_iou_thres, score_thr=def_conf_thres)
            if (type(dets) == np.ndarray):
                final_boxes, final_scores, final_cls_inds = dets[:, :4], dets[:, 4], dets[:, 5]
                result = [final_boxes, final_scores, final_cls_inds]
                self.postprocess(img, result, save_path, still_image=True)
                if cv2.waitKey(1) == ord('q'):
                    break
        self.video.release()
        if isinstance(self.vid_writer, cv2.VideoWriter):
            self.vid_writer.release()  
        cv2.destroyAllWindows()
        return

    def do_image_infer(self, filename="", target_folder=""):
        """
        To inference from image file  

        Args:
            filename : image file name  
        Returns: 
            none
        """
        save_path = myutil.get_fullpath(target_folder, filename)
        if filename == "":
            i_file = self.img_folder
        else:
            i_file = filename
        print("filename %s, target_folder= %s" % (filename, target_folder))
        print("img file = %s" % i_file)
        img = cv2.imread(i_file)
        if img is None:
            print("input file open error!!!")
            return
        # preprocessing
        x, ratio = self.preprocess(img)
        #run
        tvm_output = self.executor(tvm.nd.array(x.astype(self.dtype)))
        tvm_output = tvm_output[-1]
        # .numpy()
        tvm_output = tvm_output.numpy()
        tvm_output = np.squeeze(tvm_output).T
        # interpreter ret value
        scores = np.max(tvm_output[:,4:], axis=1)
        predictions = tvm_output[scores > def_conf_thres, :]
        scores = scores[scores > def_conf_thres]
        class_ids = np.argmax(predictions[:, 4:], axis=1)
        boxes = predictions[:, :4]

        input_shape = np.array([self.width, self.height, self.width, self.height])
        boxes = np.divide(boxes, input_shape, dtype=np.float32)
        boxes *= np.array([self.width, self.height, self.width, self.height])
        boxes = boxes.astype(np.int32)

        #predictions = np.reshape(data, (1, tsize, -1))
        # predictions = predictions.tolist()
        # 1, 84, 8400
        # scores = predictions[:, 4:tsize]
        #
        boxes_xyxy = xywh2xyxy(boxes)
        for i in range(len(boxes_xyxy)):
            boxes_xyxy[i, 0] = int(boxes_xyxy[i, 0] / ratio)
            boxes_xyxy[i, 1] = int(boxes_xyxy[i, 1] / ratio)
            boxes_xyxy[i, 2] = int(boxes_xyxy[i, 2] / ratio)
            boxes_xyxy[i, 3] = int(boxes_xyxy[i, 3] / ratio)

        dets = self.multiclass_nms(boxes_xyxy, scores, class_ids, nms_thr=def_iou_thres, score_thr=def_conf_thres)
        if (type(dets) == np.ndarray):
            final_boxes, final_scores, final_cls_inds = dets[:, :4], dets[:, 4], dets[:, 5]
            result = [final_boxes, final_scores, final_cls_inds]
            self.postprocess(img, result, save_path, still_image=True)
        return



    def run(self):
        """
        To call inference fuction  

        Args:
            none
        Returns: 
            none
        """
        f_type = myutil.check_file_type(self.img_folder)
        if self.output_location == 0 or self.output_location == 1:
            self.save_folder = ""
        else:
            self.save_folder = self.output_location 
        if f_type == "camera":
            self.do_camera_infer(self.img_folder, self.save_folder)
        elif f_type == "url":  
            self.do_url_infer(self.img_folder, self.save_folder)
        elif f_type == "video":
            self.do_video_infer(self.img_folder, self.save_folder)
        elif f_type == "image":
            self.do_image_infer(self.img_folder, self.save_folder)
        elif f_type == "directory": 
            for i, filename in enumerate(os.listdir(self.img_folder)):
                full_name = os.path.join(self.img_folder, filename)
                self.do_image_infer(full_name, self.save_folder)
        elif f_type == "unknown":
            print("unkown input!! Halt!!!")
        return

    def postprocess(self, image, label, save_path, still_image=False, vid_on=False):
        """
        To postprocessing after inference  

        Args:
            image : original image that was input image for the neural 
                    network model
            label : label infomation 
            save_path : the folder name to save a image that has 
                    object detection info. 
            still_image : whether the original image is read 
                    from a image file or not  
        Returns: 
            none
        """
        boxes, scores, cls_ids = label
        _COLORS = self.rainbow_fill(80).astype(np.float32).reshape(-1, 3)
        for i in range(len(boxes)):
            box = boxes[i]
            cls_id = int(cls_ids[i])
            score = scores[i]
            if score < self.conf_thres: 
                continue
            x0 = int(box[0])
            y0 = int(box[1])
            x1 = int(box[2])
            y1 = int(box[3])
            color = (_COLORS[cls_id] * 255).astype(np.uint8).tolist()
            text = '{}:{:.1f}%'.format(self.classes[cls_id], score * 100)
            txt_color = (0, 0, 0) if np.mean(_COLORS[cls_id]) > 0.5 else (255, 255, 255)
            font = cv2.FONT_HERSHEY_SIMPLEX
            txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]
            cv2.rectangle(image, (x0, y0), (x1, y1), color, 2)
            txt_bk_color = (_COLORS[cls_id] * 255 * 0.7).astype(np.uint8).tolist()
            cv2.rectangle(
                image,
                (x0, y0 + 1),
                (x0 + txt_size[0] + 1, y0 + int(1.5 * txt_size[1])),
                txt_bk_color,
                -1
            )
            cv2.putText(image, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)

        if self.view_img:
            cv2.imshow("result", image)
            if (vid_on == True):
                cv2.waitKey(1)
            else:
                cv2.waitKey(5000)
            # time.sleep(1)
        elif self.save_img:
            if still_image:
                cv2.imwrite(str(save_path), image)
            else:
                if self.vid_path != save_path: 
                    self.vid_path = save_path
                    if isinstance(self.vid_writer, cv2.VideoWriter):
                        self.vid_writer.release()  
                    if self.video:  
                        fps = self.video.get(cv2.CAP_PROP_FPS)
                        w, h = image.shape[1], image.shape[0]
                    else:  
                        fps, w, h = 10, image.shape[1], image.shape[0]
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    self.vid_writer = cv2.VideoWriter(save_path, 
                            fourcc, fps, (w, h))
                self.vid_writer.write(image)
        elif self.stream_out:
            # if you want to send detection results to internet
            # write proper codes here to do it
            pass
        return




if __name__ == "__main__":
    mytvm = TVMRun( 
            dev_type=def_dev_type,
            input_location= def_input_location, 
            confthr=def_conf_thres, 
            iouthr=def_iou_thres, 
            output_location= def_output_location
            )
    mytvm.load_model()
    mytvm.run()
