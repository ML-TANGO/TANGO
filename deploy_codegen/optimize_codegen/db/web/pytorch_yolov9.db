# def_label_yaml = "dataset.yaml"
# def_input_location = "./" # number=camera, url, or file_path
# def_output_location = 'result' # 0 # "./result" # 0=screen, 1=text, url,or folder_path
# def_conf_thres = 0.3
# def_iou_thres = 0.4
# def_pt_file = 'yolov9-m.torchscript'
# def_dev = "cpu" # def_dev = "cuda:0"



def_width = 640
def_height = 640

########################################################
########################################################
import os
import glob
import math
import time
import datetime
import argparse
from pathlib import Path
import cv2
import torch
import torchvision
import torch.nn as nn
import numpy as np
import json
import yaml
import sys


# image loader -----------------------------------------------------------------
img_formats = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # acceptable image suffixes
vid_formats = ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']  # acceptable video suffixes



class PyTorchRun():
    def __init__(self, model_path=def_pt_file, lyaml=def_label_yaml,
            input_location=def_input_location, confthr=def_conf_thres,
            iouthr=def_iou_thres, output_location=def_output_location):
        self.model_path = model_path
        self.inputs = []
        self.outputs = []
        self.allocations = []
        self.classes = None
        self.label_yaml =  lyaml
        self.img_folder = input_location
        self.conf_thres = confthr
        self.iou_thres = iouthr
        self.output_location = output_location
        self.video = 0
        self.vid_writer = 0
        self.vid_path = ""
        self.text_out = False
        self.view_img = False
        self.save_img = False
        self.stream_out = False
        self.imgsz = (def_width, def_height)
        if self.output_location == 0:
            self.view_img = True
        elif self.output_location == 1:
            self.text_out = True
        elif "://" in self.output_location:
            self.stream_out = True
        else:
            self.save_img = True
        with open(self.label_yaml) as f:
            classes = yaml.safe_load(f)
            self.classes = classes['names']
        self.device = torch.device(def_dev)
        self.imgsz = def_width
        self.init_model(self.model_path)
        return


    def init_model(self, weights):
        weights = self.check_file(weights)     

        # Initialize ---------------------------------------------------------------
        self.device = self.select_device()
        self.half = False # device.type != 'cpu'  # half precision only supported on CUDA
        print('device = ', self.device)
          
        # Load model ---------------------------------------------------------------
        self.model = LoadModel(weights=weights, device=self.device)  # load FP32 model
        self.stride = self.model.stride
        # Get names and colors -----------------------------------------------------
        self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names
        self.colors = [[np.random.randint(0, 255) for _ in range(3)] for _ in self.names]
        # warmup -------------------------------------------------------------------
        print('.'*50+'warmup'+'.'*50)
        self.model.warmup(imgsz=(1, 3, self.imgsz, self.imgsz))

    def LoadImage(self, img0):
        # Padded resize
        img = self.letterbox(img0, self.imgsz, stride=self.stride, auto=False)[0]

        # Convert
        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416
        img = np.ascontiguousarray(img)
        # Image tensor ---------------------------------------------------------
        img = torch.from_numpy(img).to(self.device)
        img = img.half() if self.half else img.float()  # uint8 to fp16/32
        img /= 255.0  # 0 - 255 to 0.0 - 1.0
        if img.ndimension() == 3:
            img = img.unsqueeze(0) # make a tensor with bs=1
        return img

    def do_oneimage(self, orgimg):
        img = self.LoadImage(orgimg)

        # t0 = time.time()
        # Run inference ============================================================
        t1 = self.time_synchronized()
        with torch.no_grad():   # Calculating gradients would cause a GPU memory leak
            pred = self.model(img)
            if isinstance(pred, (list, tuple)):
                pred = pred[-1] # for dual or triple heads
        # print(f"\t1. predict  : {pred.shape}")
        t2 = self.time_synchronized()

        # Apply NMS ------------------------------------------------------------
        pred = self.non_max_suppression_v9(pred) #, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)
        # print(f"\t2. nms      : {len(pred)}, {pred[0].shape}")
        t3 = self.time_synchronized()
    
        # Process detections ---------------------------------------------------
        for i, det in enumerate(pred):  # detections per image
            # print(f"\t3. draw boxes: {det}")
            gn = torch.tensor(orgimg.shape)[[1, 0, 1, 0]]  # normalization gain whwh
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = self.scale_coords(img.shape[2:], det[:, :4], orgimg.shape).round()
                # Print results
                s = ''
                for c in det[:, -1].unique():
                    n = (det[:, -1] == c).sum()  # detections per class
                    s += f"{n} {self.names[int(c)]}{'s' * (n > 1)}, "  # add to string
                # Write results
                for *xyxy, conf, cls in reversed(det):
                    label = f'{self.names[int(cls)]} {conf:.2f}'
                    im0 = self.plot_one_box(xyxy, orgimg, label=label, color=self.colors[int(cls)], line_thickness=1)
        # print(f'Done. ({time.time() - t0:.3f}s)')
        return im0

    def check_file(self, file):
        # Search for file if not found
        if Path(file).is_file() or file == '':
            return file
        else:
            files = glob.glob('./**/' + file, recursive=True)  # find file
            assert len(files), f'File Not Found: {file}'  # assert file was found
            assert len(files) == 1, f"Multiple files match '{file}', specify exact path: {files}"  # assert unique
            return files[0]  # return file
    
    def select_device(self, device='', batch_size=None):
        # device = 'cpu' or '0' or '0,1,2,3'
        s = f'TANGO {self.date_modified()} torch {torch.__version__} '  # string
        cpu = device.lower() == 'cpu'
        if cpu:
            os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False
        elif device:  # non-cpu device requested
            os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable
            assert torch.cuda.is_available(), f'CUDA unavailable, invalid device {device} requested'  # check availability
    
        cuda = not cpu and torch.cuda.is_available()
        if cuda:
            n = torch.cuda.device_count()
            if n > 1 and batch_size:  # check that batch_size is compatible with device_count
                assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'
            space = ' ' * len(s)
            for i, d in enumerate(device.split(',') if device else range(n)):
                p = torch.cuda.get_device_properties(i)
                s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2}MB)\n"  # bytes to MB
        else:
            s += 'CPU\n'
    
        # print(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe
        return torch.device('cuda:0' if cuda else 'cpu')
    
    def date_modified(self, path=__file__):
        # return human-readable file modification date, i.e. '2021-3-26'
        t = datetime.datetime.fromtimestamp(Path(path).stat().st_mtime)
        return f'{t.year}-{t.month}-{t.day}'
    
    def non_max_suppression_v9(self,
            prediction,
            conf_thres=0.25,
            iou_thres=0.45,
            classes=None,
            agnostic=False,
            multi_label=False,
            labels=(),
            max_det=300,
            nm=0,  # number of masks
    ):
        """Non-Maximum Suppression (NMS) on inference results to reject overlapping detections
    
        Returns:
             list of detections, on (n,6) tensor per image [xyxy, conf, cls]
        """
    
        if isinstance(prediction, (list, tuple)):  # YOLO model in validation model, output = (inference_out, loss_out)
            prediction = prediction[0]  # select only inference output
    
        device = prediction.device
        mps = 'mps' in device.type  # Apple MPS
        if mps:  # MPS not fully supported yet, convert tensors to CPU before NMS
            prediction = prediction.cpu()
        bs = prediction.shape[0]  # batch size
        nc = prediction.shape[1] - nm - 4  # number of classes
        mi = 4 + nc  # mask start index
        xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates
    
        # Checks
        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'
        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'
    
        # print(f"bs={bs}, nc={nc}, mi={mi}, xc={xc}")
        # Settings
        # min_wh = 2  # (pixels) minimum box width and height
        max_wh = 7680  # (pixels) maximum box width and height
        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()
        time_limit = 2.5 + 0.05 * bs  # seconds to quit after
        redundant = True  # require redundant detections
        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)
    
        t = time.time()
        output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs
        for xi, x in enumerate(prediction):  # image index, image inference
            # Apply constraints
            # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height
            x = x.T[xc[xi]]  # confidence
    
            # Cat apriori labels if autolabelling
            if labels and len(labels[xi]):
                lb = labels[xi]
                v = torch.zeros((len(lb), nc + nm + 5), device=x.device)
                v[:, :4] = lb[:, 1:5]  # box
                v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls
                x = torch.cat((x, v), 0)
    
            # If none remain process next image
            if not x.shape[0]:
                continue
    
            # Detections matrix nx6 (xyxy, conf, cls)
            box, cls, mask = x.split((4, nc, nm), 1)
            box = self.xywh2xyxy(box)  # center_x, center_y, width, height) to (x1, y1, x2, y2)
            if multi_label:
                i, j = (cls > conf_thres).nonzero(as_tuple=False).T
                x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)
            else:  # best class only
                conf, j = cls.max(1, keepdim=True)
                x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]
    
            # Filter by class
            if classes is not None:
                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]
    
            # Apply finite constraint
            # if not torch.isfinite(x).all():
            #     x = x[torch.isfinite(x).all(1)]
    
            # Check shape
            n = x.shape[0]  # number of boxes
            if not n:  # no boxes
                continue
            elif n > max_nms:  # excess boxes
                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence
            else:
                x = x[x[:, 4].argsort(descending=True)]  # sort by confidence
    
            # Batched NMS
            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes
            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores
            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS
            if i.shape[0] > max_det:  # limit detections
                i = i[:max_det]
            output[xi] = x[i]
            if mps:
                output[xi] = output[xi].to(device)
            if (time.time() - t) > time_limit:
                print(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')
                break  # time limit exceeded
    
        return output
    
    def time_synchronized(self):
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        return time.time()
    
    def scale_coords(self, img1_shape, coords, img0_shape, ratio_pad=None):
        # Rescale coords (xyxy) from img1_shape to img0_shape
        if ratio_pad is None:  # calculate from img0_shape
            gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new
            pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding
        else:
            gain = ratio_pad[0][0]
            pad = ratio_pad[1]
    
        coords[:, [0, 2]] -= pad[0]  # x padding
        coords[:, [1, 3]] -= pad[1]  # y padding
        coords[:, :4] /= gain
        self.clip_coords(coords, img0_shape)
        return coords
    
    def clip_coords(self, boxes, img_shape):
        # Clip bounding xyxy bounding boxes to image shape (height, width)
        boxes[:, 0].clamp_(0, img_shape[1])  # x1
        boxes[:, 1].clamp_(0, img_shape[0])  # y1
        boxes[:, 2].clamp_(0, img_shape[1])  # x2
        boxes[:, 3].clamp_(0, img_shape[0])  # y2
    
    def xywh2xyxy(self, x):
        # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right
        y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
        y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x
        y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y
        y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x
        y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y
        return y
    
    def plot_one_box(self, x, img, color=None, label=None, line_thickness=3):
        # Plots one bounding box on image img
        tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness
        color = color or [np.random.randint(0, 255) for _ in range(3)]
        c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))
        cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)
        if label:
            tf = max(tl - 1, 1)  # font thickness
            t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]
            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3
            cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled
            cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)
        return img

    def letterbox(self, img, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True, stride=32):
        # Resize and pad image while meeting stride-multiple constraints
        shape = img.shape[:2]  # current shape [height, width]
        if isinstance(new_shape, int):
            new_shape = (new_shape, new_shape)
    
        # Scale ratio (new / old)
        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
        if not scaleup:  # only scale down, do not scale up (for better test mAP)
            r = min(r, 1.0)
    
        # Compute padding
        ratio = r, r  # width, height ratios
        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding
        if auto:  # minimum rectangle
            dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding
        elif scaleFill:  # stretch
            dw, dh = 0.0, 0.0
            new_unpad = (new_shape[1], new_shape[0])
            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios
    
        dw /= 2  # divide padding into 2 sides
        dh /= 2
    
        if shape[::-1] != new_unpad:  # resize
            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
        return img, ratio, (dw, dh)
    
    
# model loader -----------------------------------------------------------------
class LoadModel(nn.Module):
    def __init__(self, weights='bestmodel.torchscript', device=torch.device('cpu')):
        super().__init__()
        w = str(weights[0] if isinstance(weights, list) else weights)
        fp16 = False
        stride = 32  # default stride
        cuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA
        print(f'Loading {w} for TorchScript inference...')
        extra_files = {'config.txt': ''}  # model metadata
        model = torch.jit.load(w, _extra_files=extra_files, map_location=device)
        # model.half() if fp16 else model.float()
        model.float()
        if extra_files['config.txt']:  # load metadata dict
            d = json.loads(extra_files['config.txt'],
                            object_hook=lambda d: {int(k) if k.isdigit() else k: v
                                                    for k, v in d.items()})
            self.stride, self.names = int(d['stride']), d['names']
        model.eval()
        self.__dict__.update(locals())  # assign all variables to self

    def forward(self, im):
        if self.fp16 and im.dtype != torch.float16:
            im = im.half()  # to FP16

        y = self.model(im)

        if isinstance(y, (list, tuple)):
            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]
        else:
            return self.from_numpy(y)

    def from_numpy(self, x):
        return torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x

    def warmup(self, imgsz=(1, 3, 640, 640)):
        # Warmup model by running inference once
        # warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton
        # if any(warmup_types) and (self.device.type != 'cpu' or self.triton):
        im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input
        for _ in range(2): # if self.jit else 1): 
            self.forward(im)  # warmup

if __name__ == "__main__":
    myTorch = PyTorchRun(model_path= def_pt_file,
            input_location= def_input_location,
            confthr=def_conf_thres,
            iouthr=def_iou_thres,
            output_location= def_output_location
            )
    myTorch.run()

