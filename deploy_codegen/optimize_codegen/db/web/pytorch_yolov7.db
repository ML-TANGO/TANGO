# def_label_yaml = "dataset.yaml"
# def_input_location = "./images" # number=camera, url, or file_path
# def_output_location = 'result' # 0 # "./result" # 0=screen, 1=text, url,or folder_path
# def_conf_thres = 0.3
# def_iou_thres = 0.4
# def_pt = "yolov7_tiny.pt"
# def_dev = "cpu" 
# def_dev = "cuda:0"
# def_width = 640
# def_height = 640

""" copyright notice
This module is for testing code for neural network model.
"""
###########################################################
###########################################################
import cv2
import numpy as np
import time
import yaml
import os
import sys
import myutil
import matplotlib.pyplot as plt
import torch
import torchvision
import yoloe_core
from yoloe_core.yolov7_utils.models.experimental import attempt_load
from yoloe_core.yolov7_utils.utils.general import check_img_size, scale_coords
from yoloe_core.yolov7_utils.utils.plots import plot_one_box
# import pycuda.autoinit
# import pycuda.driver as cuda


###############################################################
def xywh2xyxy(x):
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x
    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y
    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x
    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y
    return y


###############################################################
# Class definition for PyTorch run module  
###############################################################
class PyTorchRun():
    def __init__(self, model_path=def_pt, lyaml=def_label_yaml, 
            input_location=def_input_location, confthr=def_conf_thres, 
            iouthr=def_iou_thres, output_location=def_output_location):
        self.model_path = model_path
        self.inputs = []
        self.outputs = []
        self.allocations = []
        self.classes = None
        self.label_yaml =  lyaml
        self.img_folder = input_location 
        self.conf_thres = confthr 
        self.iou_thres = iouthr 
        self.output_location = output_location 
        self.video = 0
        self.vid_writer = 0
        self.vid_path = ""
        self.text_out = False
        self.view_img = False
        self.save_img = False
        self.stream_out = False
        self.imgsz = (def_width, def_height)
        if self.output_location == 0:
            self.view_img = True
        elif self.output_location == 1:
            self.text_out = True
        elif "://" in self.output_location:
            self.stream_out = True
        else:
            self.save_img = True
        with open(self.label_yaml) as f:
            classes = yaml.safe_load(f)
            self.classes = classes['names']
        self.device = torch.device(def_dev)
        self.imgsz = def_width
        self.model = self.init_model(def_pt)
        return
        

    def init_model(self, pt_path):
        """
        To preprocess image for neural network input 

        Args:
            org_img : image data 
        Returns: 
            preprocessed image data
        """
        model = attempt_load(pt_path, map_location=self.device) 
        stride = int(model.stride.max()) 
        self.imgsz = check_img_size(self.imgsz, s=stride) 
        names = model.names 
        # colors = [[np.random.randint(0, 255) for _ in range(3)] for _ in names]
        if self.device != torch.device('cpu'):
            model(torch.zeros(1, 3, self.imgsz, self.imgsz).to(self.device).type_as(next(model.parameters())))  
        return model


    def detect(self, img, ratio):
        """
        To preprocess image for neural network input 

        Args:
            org_img : image data 
        Returns: 
            preprocessed image data
        """
        old_img_w = old_img_h = self.imgsz
        old_img_b = 1
        if self.device.type != torch.device('cpu') and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):
            old_img_b = img.shape[0]
            old_img_h = img.shape[2]
            old_img_w = img.shape[3]
            for i in range(3):
                model(img, augment=True)[0]
        with torch.no_grad():  
            pred = self.model(img)[0]
        dets = self.multiclass_nms(pred, ratio)
        return dets


    def preprocess(self, image):
        """
        To preprocess image for neural network input 

        Args:
            org_img : image data 
        Returns: 
            preprocessed image data
        """
        swap=(2, 0, 1)
        if len(image.shape) == 3:
            padded_img = np.ones((self.imgsz, self.imgsz, 3)) * 114.0
        else:
            padded_img = np.ones(self.imgsz) * 114.0
        img = np.array(image)
        r = min(self.imgsz / img.shape[0], self.imgsz / img.shape[1])
        resized_img = cv2.resize(
            img,
            (int(img.shape[1] * r), int(img.shape[0] * r)),
            interpolation=cv2.INTER_LINEAR,
        ).astype(np.float32)
        padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img
        padded_img = padded_img[:, :, ::-1]
        padded_img /= 255.0
        padded_img = padded_img.transpose(swap)
        padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)
        t = torch.from_numpy(padded_img).to(torch.device(def_dev))
        t = t.unsqueeze(0)
        return t, r


    def do_camera_infer(self, dev=0, target_folder=""):
        """
        To inference from camera input  

        Args:
            dev : camera device number (defualt: 0)
        Returns: 
            none
        """
        save_path = myutil.get_fullpath(self.save_folder, filename)
        self.video = cv2.VideoCapture(dev)
        self.video.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.video.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        print("Press q to quit")
        while (self.video.isOpened()):
            flag, img = self.video.read()
            if flag == False:
                break
            preproc_image, ratio  = self.preprocess(img)
            
            # inference
            dets = self.detect(preproc_image, ratio)
        
            # get bbox image
            if (type(dets) == np.ndarray):
                bbox_img = self.get_bboximg(img, dets)
            else:
                bbox_img = img

            # save or image show
            self.postprocess(bbox_img, save_path)
            if cv2.waitKey(1) == ord('q'):
                break
        self.video.release()
        if isinstance(self.vid_writer, cv2.VideoWriter):
            self.vid_writer.release()  
        cv2.destroyAllWindows()
        return


    def do_url_infer(self, url, target_folder=""):
        """
        To inference from streamin data  

        Args:
            url : url for streaming input 
        Returns: 
            none
        """
        save_path = myutil.get_fullpath(target_folder, filename)
        self.video = cv2.VideoCapture(url)
        print("Press q to quit")
        while (self.video.isOpened()):
            flag, img = self.video.read()
            if flag == False:
                break
            preproc_image, ratio  = self.preprocess(img)
            
            # inference
            dets = self.detect(preproc_image, ratio)
        
            # get bbox image
            if (type(dets) == np.ndarray):
                bbox_img = self.get_bboximg(img, dets)
            else:
                bbox_img = img

            # save or image show
            self.postprocess(bbox_img, save_path)
            if cv2.waitKey(1) == ord('q'):
                break

        self.video.release()
        if isinstance(self.vid_writer, cv2.VideoWriter):
            self.vid_writer.release()  
        cv2.destroyAllWindows()
        return


    def do_video_infer(self, filename, target_folder=""):
        """
        To inference from video file  

        Args:
            filename : video file name  
        Returns: 
            none
        """
        save_path = myutil.get_fullpath(target_folder, filename)
        if filename == "":
            i_file = self.img_folder
        else:
            i_file = filename
        self.video = cv2.VideoCapture(i_file)
        print("Press q to quit")
        while (self.video.isOpened()):
            flag, img = self.video.read()
            if flag == False:
                break
            preproc_image, ratio  = self.preprocess(img)
            
            # inference
            dets = self.detect(preproc_image, ratio)
        
            # get bbox image
            if (type(dets) == np.ndarray):
                bbox_img = self.get_bboximg(img, dets)
            else:
                bbox_img = img

            # save or image show
            self.postprocess(bbox_img, save_path)
            if cv2.waitKey(1) == ord('q'):
                break
        self.video.release()
        if isinstance(self.vid_writer, cv2.VideoWriter):
            self.vid_writer.release()  
        cv2.destroyAllWindows()
        return


    def do_image_infer(self, filename="", target_folder=""):
        """
        To inference from image file  

        Args:
            filename : image file name  
        Returns: 
            none
        """
        save_path = myutil.get_fullpath(target_folder, filename)
        if filename == "":
            i_file = self.img_folder
        else:
            i_file = filename
        print("filename %s, target_folder= %s" % (filename, target_folder))
        print("img file = %s" % i_file)
        
        # get image
        img = cv2.imread(i_file)
        if img is None:
            print("input file open error!!!")
            return
            
        # preprocess
        preproc_image, ratio  = self.preprocess(img)
        
        # inference
        dets = self.detect(preproc_image, ratio)
        
        # get bbox image
        if (type(dets) == np.ndarray):
            bbox_img = self.get_bboximg(img, dets)
        else:
            bbox_img = img

        # save or image show
        self.postprocess(bbox_img, save_path, still_image=True)
        return
        
        
    def do_oneimage(self, orgimg):
        """
        To inference from image data 

        Args:
            filename : image data 
        Returns: 
            bboximage, detectioninfo
        """           
        # preprocess
        preproc_image, ratio  = self.preprocess(orgimg)
        
        # inference
        dets = self.detect(preproc_image, ratio)
        
        # get bbox image
        if (type(dets) == np.ndarray):
            bbox_img = self.get_bboximg(orgimg, dets)
        else:
            bbox_img = orgimg
        return bbox_img
        

    def nms(self, boxes, scores, nms_thr):
        x1 = boxes[:, 0]
        y1 = boxes[:, 1]
        x2 = boxes[:, 2]
        y2 = boxes[:, 3]
        areas = (x2 - x1 + 1) * (y2 - y1 + 1)
        order = scores.argsort()[::-1]
        # order = scores[np.argsort(-scores)]
        # size = len(order)
        keep = []
        while order.size > 0:
        # for k in range(size):  
            i = order[0]
            keep.append(i)
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])
            w = np.maximum(0.0, xx2 - xx1 + 1)
            h = np.maximum(0.0, yy2 - yy1 + 1)
            inter = w * h
            ovr = inter / (areas[i] + areas[order[1:]] - inter)
            inds = np.where(ovr <= nms_thr)[0]
            order = order[inds + 1]
        return keep


    def multiclass_nms(self, res, ratio):
        data = res.detach().numpy()
        predictions = np.reshape(data, (1, -1, int(5+len(self.classes))))[0]
        boxes = predictions[:, :4]
        scores = predictions[:, 4:5] * predictions[:, 5:]
        boxes_xyxy = np.ones_like(boxes)
        boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2] / 2.
        boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3] / 2.
        boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2] / 2.
        boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3] / 2.
        boxes_xyxy /= ratio
        
        nms_thr = self.iou_thres 
        score_thr = self.conf_thres
        
        final_dets = []
        num_classes = scores.shape[1]
        for cls_ind in range(num_classes):
            cls_scores = scores[:, cls_ind]
            valid_score_mask = cls_scores > score_thr
            if valid_score_mask.sum() == 0:
                continue
            else:
                valid_scores = cls_scores[valid_score_mask]
                valid_boxes = boxes_xyxy[valid_score_mask]
                keep = self.nms(valid_boxes, valid_scores, nms_thr)
                if len(keep) > 0:
                    cls_inds = np.ones((len(keep), 1)) * cls_ind
                    dets = np.concatenate(
                        [valid_boxes[keep], valid_scores[keep, None], cls_inds], 1
                    )
                    final_dets.append(dets)
        if len(final_dets) == 0:
            print("None")
            return None
        return np.concatenate(final_dets, 0)


    def run(self):
        """
        To call inference fuction  

        Args:
            none
        Returns: 
            none
        """
        f_type = myutil.check_file_type(self.img_folder)
        if self.output_location == 0 or self.output_location == 1:
            self.save_folder = ""
        else:
            self.save_folder = self.output_location 
        if f_type == "camera":
            self.do_camera_infer(self.img_folder, self.save_folder)
        elif f_type == "url":  
            self.do_url_infer(self.img_folder, self.save_folder)
        elif f_type == "video":
            self.do_video_infer(self.img_folder, self.save_folder)
        elif f_type == "image":
            self.do_image_infer(self.img_folder, self.save_folder)
        elif f_type == "directory": 
            for i, filename in enumerate(os.listdir(self.img_folder)):
                full_name = os.path.join(self.img_folder, filename)
                self.do_image_infer(full_name, self.save_folder)
        elif f_type == "unknown":
            print("unkown input!! Halt!!!")
        return

    def rainbow_fill(self, size=50):  # simpler way to generate rainbow color
        cmap = plt.get_cmap('jet')
        color_list = []
        for n in range(size):
            color = cmap(n/size)
            color_list.append(color[:3])  # might need rounding? (round(x, 3) for x in color)[:3]
        return np.array(color_list)

    def get_bboximg(self, image, dets):
        """
        To make bbox after inference  

        Args:
            image : original image that was input image for the neural 
                    network model
            label : label infomation 
            save_path : the folder name to save a image that has 
                    object detection info. 
            still_image : whether the original image is read 
                    from a image file or not  
        Returns: 
            none
        """
        boxes, scores, cls_ids = dets[:, :4], dets[:, 4], dets[:, 5]
        image = np.squeeze(image)
        # image *= 255
        image = image.astype(np.uint8)
        _COLORS = self.rainbow_fill(80).astype(np.float32).reshape(-1, 3)
        for i in range(len(boxes)):
            box = boxes[i]
            cls_id = int(cls_ids[i])
            score = scores[i]
            if score < self.conf_thres: 
                continue
            x0 = int(box[0])
            y0 = int(box[1])
            x1 = int(box[2])
            y1 = int(box[3])
            color = (_COLORS[cls_id] * 255).astype(np.uint8).tolist()
            text = '{}:{:.1f}%'.format(self.classes[cls_id], score * 100)
            txt_color = (0, 0, 0) if np.mean(_COLORS[cls_id]) > 0.5 else (255, 255, 255)
            font = cv2.FONT_HERSHEY_SIMPLEX
            txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]
            cv2.rectangle(image, (x0, y0), (x1, y1), color, 2)
            txt_bk_color = (_COLORS[cls_id] * 255 * 0.7).astype(np.uint8).tolist()
            cv2.rectangle(
                image,
                (x0, y0 + 1),
                (x0 + txt_size[0] + 1, y0 + int(1.5 * txt_size[1])),
                txt_bk_color,
                -1
            )
            cv2.putText(image, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)
        return image


    def postprocess(self, image, save_path, still_image=False):
        if self.view_img:
            cv2.imshow("result", image)
            cv2.waitKey(1)
            time.sleep(1)
        elif self.save_img:
            if still_image:
                cv2.imwrite(str(save_path), image)
            else:
                if self.vid_path != save_path: 
                    self.vid_path = save_path
                    if isinstance(self.vid_writer, cv2.VideoWriter):
                        self.vid_writer.release()  
                    if self.video:  
                        fps = self.video.get(cv2.CAP_PROP_FPS)
                        w, h = image.shape[1], image.shape[0]
                    else:  
                        fps, w, h = 10, image.shape[1], image.shape[0]
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    self.vid_writer = cv2.VideoWriter(save_path, 
                            fourcc, fps, (w, h))
                self.vid_writer.write(image)
        elif self.stream_out:
            # if you want to send detection results to internet
            # write proper codes here to do it
            pass
        return


if __name__ == "__main__":
    myTorch = PyTorchRun(model_path=def_pt, 
            input_location= def_input_location, 
            confthr=def_conf_thres, 
            iouthr=def_iou_thres, 
            output_location= def_output_location
            )
    myTorch.run()
