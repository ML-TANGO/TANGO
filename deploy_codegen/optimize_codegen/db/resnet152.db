#!/usr/bin/python
# -*- coding: utf-8 -*-
# DEF_IMG_PATH = "./test"
# DEF_ACC = "cpu"
# DEF_PT_FILE = "resnet152.pt"

###########################################################################
# Pytorch - Medical Chest X-ray classification Test
###########################################################################
_description = '''\
        Medial Chect X-ray
'''
import argparse
import textwrap
import os
import torch
from torchvision import datasets
from torch.utils.data import DataLoader
from torchvision.transforms import Compose, Normalize, ToTensor
import pprint
import pandas as pd
import numpy as np

###########################################################################
###################### for classification setting##########################
###########################################################################
import albumentations as A
from albumentations.pytorch import ToTensorV2
import torchvision.transforms as T

num_classes = 2
data_folder = "/home/seongwon/PycharmProjects/data"
train_root = "/chest_xray/train/"
test_root = "/chest_xray/test/"
# lr scheduler
lr_scheduler = True
# load state dict from model
use_pretrained = False
pre_model_path = ""
# augmentation
# custom
# training
custom_transform = A.Compose(
    [
        A.Resize(height=256, width=256),
        A.Affine(rotate=(-10, 10), scale=(0.8, 1.2)),
        A.OneOf(
            [
                A.ElasticTransform(
                    p=1, alpha=60, sigma=120 * 0.05, alpha_affine=120 * 0.03
                ),
                A.GridDistortion(p=1),
                A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),
            ],
            p=0.5,
        ),
        A.HorizontalFlip(p=0.5),
        A.Normalize(mean=0.5, std=0.5),
        ToTensorV2(),
    ]
)


# testdata
custom_test = A.Compose(
    [
        A.Resize(height=256, width=256),
         A.Normalize(mean=0.5, std=0.5),
        ToTensorV2(),
    ]
)


# torch transform
transforms_T = T.Compose(
    [T.Resize((256, 256)), T.ToTensor(), T.Normalize((0.5,), (0.5,)),]
)


###########################################################################
###################### for setup  #########################################
###########################################################################
from email.policy import strict
import torch.nn as nn
from torch import optim
from torchvision import datasets
from torchvision.models import resnet, vgg, inception
from torch.utils.data import DataLoader
from torch.utils import model_zoo
import math
from torch.optim.lr_scheduler import _LRScheduler

model_urls = {
    "resnet50": "https://download.pytorch.org/models/resnet50-0676ba61.pth",
    "densenet121": "https://download.pytorch.org/models/densenet121-a639ec97.pth",
}


class MyDataset:
    def __init__(
        self, *, data_src=data_folder, batch_size, dataset
    ):
        self.dataset = dataset
        self.data_src = data_src
        self.batch_size = batch_size

    # dataset select
    def load_dataset(self,):
        if self.dataset == "custom":
            my_dataset_root = {
                "train": self.data_src + train_root,
                "test": self.data_src + test_root,
            }
            my_transforms = {
                "train": Transforms(custom_transform),
                "test": Transforms(custom_test),
            }
            data_dict = {
                "train": datasets.ImageFolder(
                    root=my_dataset_root["train"],
                    transform=my_transforms["train"],
                    loader=custom_pil_loader,
                ),
                "val": datasets.ImageFolder(
                    root=my_dataset_root["train"],
                    transform=my_transforms["test"],
                    loader=custom_pil_loader,
                ),
                "test": datasets.ImageFolder(
                    root=my_dataset_root["test"],
                    transform=my_transforms["test"],
                    loader=custom_pil_loader,
                ),
            }

            print("dataset load: ", len(data_dict["train"]))
            sampler_dict = train_val_split(data_dict["train"])

            if sampler_dict["test"] != None:
                test_count = len(sampler_dict["test"])
            else:
                test_count = len(data_dict["test"]) 
                
            print(
                "train val split:\n",
                "train:",
                len(sampler_dict["train"]),
                "|",
                "val:",
                len(sampler_dict["val"]),
                "|",
                "test:",
                test_count
            )

            data_loaders = {
                x: torch.utils.data.DataLoader(
                    dataset=data_dict[x],
                    batch_size=self.batch_size,
                    sampler=sampler_dict[x],
                    pin_memory=True,
                    num_workers=4,
                )
                for x in ["train", "val", "test"]
            }
        else:
            raise ValueError('please check your "dataset" input.')

        return data_loaders["train"], data_loaders["val"], data_loaders["test"]


class Initializer:
    def __init__(self, net, lr=0.1, momentum=0.9, dataset=None, device_num="cuda:0"):
        self.net = net
        self.dataset = dataset
        self.device_num = device_num
        _model, _device = self.select_model(pretrained=use_pretrained)
        self.model = _model
        self.device = _device

        self.optimizer = MyOptimizer(self.model, lr, momentum)

    def select_optimizer(self, opt):
        switch_case = {
            "SGD": self.optimizer.SGD(),
            "Adam": self.optimizer.Adam(),
            "NAG": self.optimizer.Nesterov(),
            "RMSprop": self.optimizer.RMSprop(),
        }.get(opt, "error")

        if switch_case == "error":
            raise ValueError('please check your "opt" input.')
        return switch_case

    def select_model(self, pretrained=False):
        global num_classes

        num_classes = {"custom": num_classes,}.get(
            self.dataset, "error"
        )

        model = {
            "resnet18": resnet.resnet18(num_classes=num_classes),
            "resnet50": resnet.resnet50(num_classes=num_classes),
            "resnet101": resnet.resnet101(num_classes=num_classes),
            "resnet152": resnet.resnet152(num_classes=num_classes),
            "vgg16": vgg.vgg16(num_classes=num_classes),
            # "densenet121": densenet_1ch.densenet121(num_classes=num_classes),
            # "densenet169": densenet_1ch.densenet169(num_classes=num_classes),
            # "densenet201": densenet_1ch.densenet201(num_classes=num_classes),
            # "densenet121_2048": densenet_1ch.densenet121_2048(num_classes=num_classes),
        }.get(self.net, "error")

        if model == "error":
            raise ValueError('please check your "net" input.')

        if self.dataset == "custom" and self.net[0:6] == "resnet":
            model.conv1 = nn.Conv2d(
                1, 64, kernel_size=7, stride=2, padding=3, bias=False
            )

        if pretrained:
            self.model_ = model
            self._load_pretrained(url="TODO")
            model = self.model_
            print("Pretrained weight applied.")

        device = torch.device(self.device_num if torch.cuda.is_available() else "cpu")
        model.to(device)
        print("device:", device)

        # parallel processing (under construction)
        # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # if torch.cuda.device_count() > 1:
        #     model = nn.DataParallel(model).cuda()

        return model, device

    def _load_pretrained(self, url, inchans=1):
        _ = url
        model_now = "mymodel"
        _pre_model = torch.load(pre_model_path, map_location='cuda:0')
        pretrained_dict = _pre_model
        state_dict = pretrained_dict
        self.model_.load_state_dict(state_dict, strict=False)

    def select_lossfunction(self, l_func):
        switch_case = {
            "CE": nn.CrossEntropyLoss(label_smoothing=0.1),
            "FL": FocalLoss(),
        }.get(l_func, "error")

        if switch_case == "error":
            raise ValueError("please check your (loss function) input.")
        return switch_case


class MyOptimizer:
    def __init__(self, model, lr, momentum):
        self.model = model
        self.lr = lr
        self.momentum = momentum

    def SGD(self):
        return optim.SGD(
            self.model.parameters(),
            lr=self.lr,
            momentum=self.momentum,
            weight_decay=0.0001,
        )

    def Adam(self):
        return optim.Adam(
            self.model.parameters(), lr=self.lr, weight_decay=0.0001
        )  # default lr = 0.001

    def Nesterov(self):
        return optim.SGD(
            self.model.parameters(),
            lr=self.lr,
            momentum=self.momentum,
            weight_decay=0.0005,
            nesterov=True,
        )

    def RMSprop(self):
        return optim.RMSprop(self.model.parameters(), lr=self.lr)  # default lr = 0.01


class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2, logits=False, reduce=True):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits
        self.reduce = reduce

    def forward(self, inputs, targets):
        ce_loss_ = nn.CrossEntropyLoss(reduction="none")
        ce_loss = ce_loss_(inputs, targets)
        pt = torch.exp(-ce_loss)
        F_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        if self.reduce:
            return torch.mean(F_loss)
        else:
            return F_loss


def set_lr_scheduler(optimizer, epochs, last_ep):
    if last_ep == 0:
        last_ep = -1
    decay_step = int(epochs / 10)
    lr_scheduler = CosineAnnealingWarmUpRestarts(
        optimizer, T_0=100, T_mult=2, eta_max=0.0002, T_up=10, gamma=0.5
    )
    return lr_scheduler


class CosineAnnealingWarmUpRestarts(_LRScheduler):
    def __init__(
        self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1.0, last_epoch=-1
    ):
        if T_0 <= 0 or not isinstance(T_0, int):
            raise ValueError("Expected positive integer T_0, but got {}".format(T_0))
        if T_mult < 1 or not isinstance(T_mult, int):
            raise ValueError("Expected integer T_mult >= 1, but got {}".format(T_mult))
        if T_up < 0 or not isinstance(T_up, int):
            raise ValueError("Expected positive integer T_up, but got {}".format(T_up))
        self.T_0 = T_0
        self.T_mult = T_mult
        self.base_eta_max = eta_max
        self.eta_max = eta_max
        self.T_up = T_up
        self.T_i = T_0
        self.gamma = gamma
        self.cycle = 0
        self.T_cur = last_epoch
        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)

    def get_lr(self):
        if self.T_cur == -1:
            return self.base_lrs
        elif self.T_cur < self.T_up:
            return [
                (self.eta_max - base_lr) * self.T_cur / self.T_up + base_lr
                for base_lr in self.base_lrs
            ]
        else:
            return [
                base_lr
                + (self.eta_max - base_lr)
                * (
                    1
                    + math.cos(
                        math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up)
                    )
                )
                / 2
                for base_lr in self.base_lrs
            ]

    def step(self, epoch=None):
        if epoch is None:
            epoch = self.last_epoch + 1
            self.T_cur = self.T_cur + 1
            if self.T_cur >= self.T_i:
                self.cycle += 1
                self.T_cur = self.T_cur - self.T_i
                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up
        else:
            if epoch >= self.T_0:
                if self.T_mult == 1:
                    self.T_cur = epoch % self.T_0
                    self.cycle = epoch // self.T_0
                else:
                    n = int(
                        math.log(
                            (epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult
                        )
                    )
                    self.cycle = n
                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (
                        self.T_mult - 1
                    )
                    self.T_i = self.T_0 * self.T_mult ** (n)
            else:
                self.T_i = self.T_0
                self.T_cur = epoch

        self.eta_max = self.base_eta_max * (self.gamma ** self.cycle)
        self.last_epoch = math.floor(epoch)
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group["lr"] = lr


###########################################################################
###################### for Until   ########################################
###########################################################################
import os
import yaml
from PIL import Image
import pandas as pd
import numpy as np
import requests
from torch.utils.data.sampler import SubsetRandomSampler
import cv2

def yml_to_dict(filepath):
    with open(filepath) as f:
        taskdict = yaml.load(f, Loader=yaml.FullLoader)
    return taskdict


# load image on dataloader with grayscale
def custom_pil_loader(path):
    with open(path, "rb") as f:
        img = Image.open(f)
        img.load()
        return img.convert("L")


def train_val_split(dataset):
    dataset_size = len(dataset)  # 전체크기
    indices = list(range(dataset_size))  # 전체 인덱스 리스트만들고
    split = int(np.floor(0.2 * dataset_size))  # 내림함수로 20% 지점 인덱스
    np.random.seed(22)
    np.random.shuffle(indices)  # 인덱스 리스트 섞어줌

    # 섞어진 리스트에서 처음부터 ~번째 까지 val, ~+1번째부터 끝 인덱스까지 train
    train_indices_, test_indices = indices[split:], indices[:split]

    if train_root == "/su_xray-resize/":
        np.random.seed(42)
        np.random.shuffle(train_indices_)
        split2 = int(np.floor(0.2 * len(train_indices_)))
        train_indices, val_indices = train_indices_[split2:], train_indices_[:split2]

        train_sampler = SubsetRandomSampler(train_indices)
        val_sampler = SubsetRandomSampler(val_indices)
        test_sampler = SubsetRandomSampler(test_indices)
    

    train_sampler = SubsetRandomSampler(train_indices_)
    val_sampler = SubsetRandomSampler(test_indices)
    test_sampler = None
    result_dict = {"train": train_sampler, "val": val_sampler, "test": test_sampler}
    return result_dict


def align_csv(path, align_list):
    data_frame = pd.read_csv(path)
    data_frame = data_frame[align_list]
    data_frame.to_csv(path, mode="w")
    return data_frame


def save_csv(data, path):
    df = pd.DataFrame(data)
    df = df.transpose()
    if not os.path.exists(path):
        df.to_csv(path, mode="w")
    else:
        df.to_csv(path, mode="a", header=False)


def send_alarm_to_slack(msg):
    url = ""
    payload = {"text": msg}
    requests.post(url, json=payload)


def create_directory(_dir="./runs/"):
    num = 1
    while True:
        temp = _dir + "exp" + str(num)
        if not os.path.exists(temp):
            os.makedirs(temp)
            break
        else:
            num += 1
    return temp


# apply albumentations on torch dataloader
class Transforms:
    def __init__(self, transforms: A.Compose):
        self.transforms = transforms

    def __call__(self, img, *args, **kwargs):
        return self.transforms(image=np.array(img))["image"]


###########################################################################
###################### for inference ######################################
###########################################################################
def preprocess_image(
    img: np.ndarray, mean=[
        0.5,], std=[
            0.5,]) -> torch.Tensor:
    preprocessing = Compose([
        ToTensor(),
        Normalize(mean=mean, std=std)
    ])
    return preprocessing(img.copy()).unsqueeze(0)


def ArgumentParse(_intro_msg, L_Param, bUseParam=False):
    parser = argparse.ArgumentParser(
        prog='inference.py',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent(_intro_msg))

    parser.add_argument('-i', '--image_path', default=DEF_IMG_PATH)
    parser.add_argument('-o', '--order', default="onebyone")
    parser.add_argument('-d', '--device', default=DEF_ACC) 
    parser.add_argument('-p', '--pt_path', default=DEF_PT_FILE)
    parser.add_argument('-m', '--model', default="resnet152")
    parser.add_argument('--save_dict', action='store_true')
    args = parser.parse_args()
    return args


class Inference:
    def __init__(self, L_Param=None):
        self.args       = ArgumentParse(_description, L_Param, bUseParam=False)

        _model, _device = self.initialization(device=self.args.device)
        self.model      = _model
        self.device     = _device
        print(f"\nOperate selected inference ... '{self.args.order}'\n")

    def initialization(self, device):
        initializer = Initializer(net=self.args.model,
                                  dataset="custom",
                                  device_num=device)
        model = initializer.model
        device = initializer.device
        model.load_state_dict(torch.load(self.args.pt_path,
                                         map_location=device)['model_state_dict'], strict=True)

        return model, device

    @staticmethod
    def loader_with_transforms(root="./test", batch_size=16):
        my_dataset_root = root
        _transforms = Transforms(custom_test)

        print(my_dataset_root)
        print(_transforms)
        print(custom_pil_loader)

        dataset = datasets.ImageFolder(root=my_dataset_root,
                                       transform=_transforms, loader=custom_pil_loader)
        data_loader = torch.utils.data.DataLoader(
            dataset=dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)

        return data_loader

    # class 폴더에 분류가 되어있는 경우
    def operation_all(self):
        data_loader = self.loader_with_transforms(root=self.args.image_path)
        test_accuracy, report , _l = evaluate(self.model, data_loader, self.device, is_test=True)
        print("\n=============================================")
        # pprint.pprint(report)
        print('prediction Accuracy: {:.2f}%'.format(report['accuracy']*100))
        print("=============================================\n")
        print('details')
        print("---------------------------------------------")
        print('NORMAL')
        pprint.pprint(report['NORMAL'])
        print("\n")
        print('PNEUMONIA')
        pprint.pprint(report['PNEUMONIA'])
        print("=============================================")


    def operation_onebyone(self):
        _transforms = Transforms(custom_test)

        f_list = os.listdir(self.args.image_path)
        transformed_list = []
        for file in f_list:
            img = custom_pil_loader(self.args.image_path + '/' + file)
            transformed_list.append(_transforms(img))

        label_tags = {
            0: 'Normal',
            1: 'Pneumonia',
        }
        r_list = []

        self.model.eval()

        cnt = 0
        with torch.no_grad():
            for data in transformed_list:
                print("Examine image %d -> FILE = %s" % ((cnt+1), f_list[cnt]))
                path = self.args.image_path + '/' + f_list[cnt]
                myImg = cv2.imread(path, cv2.IMREAD_COLOR)
                winname = f_list[cnt]
                cv2.namedWindow(winname)
                cv2.moveWindow(winname, 1400, 10)
                cv2.imshow(winname, myImg)
                cv2.waitKey(1000)
                output = self.model(data.unsqueeze(0).float().to(self.device))
                _score, predicted = output.max(1)
                pred = label_tags[predicted.item()]
                cnt = cnt + 1

                _score_p = torch.softmax(output, dim=1)
                print('prediction result: %s  Score: %f' %(pred, _score))
                print('softmax result : ', _score_p.max())
                print('\n')
                r_list.append([pred, _score_p.cpu()])
                input('Hit "Enter Key" to proceed....')
                cv2.destroyAllWindows()

        r_dict = dict(zip(f_list, r_list))
        path = './infer_result.csv'
        if self.args.save_dict:
            df = pd.DataFrame(r_dict)
            df = df.transpose()
            df.columns = ['predict', 'softmax value']
            if not os.path.exists(path):
                df.to_csv(path, mode="w")
            else:
                df.to_csv(path, mode="a", header=False)

        n = 0
        p = 0
        for k, v in r_dict.items():
            if v[0] == "Normal":
                n += 1
            else:
                p += 1

        print("# of images: ", len(r_dict), "\nNormal: ", n, "\nPneumonia: ", p)

    def operation_one_image(self):
        _transforms = Transforms(custom_test)
        img = custom_pil_loader(self.args.image_path)
        transformed = _transforms(img)

        label_tags = {
            0: 'Normal',
            1: 'Pneumonia',
        }
        self.model.eval()

        with torch.no_grad():
            output = self.model(transformed.unsqueeze(0).float().to(self.device))

            _score, predicted = output.max(1)
            pred = label_tags[predicted.item()]

        print('prediction result: %s  Score: %f' %(pred, _score))
        print('softmax result : ', torch.softmax(output, dim=1).max())

    def selected_operation(self,):
        if self.args.order == 'all':
            self.operation_all()
        elif self.args.order == "onebyone":
            self.operation_onebyone()
        elif self.args.order == "one":
            self.operation_one_image()
        else:
            print("Please check 'operation' input.")



if __name__ == '__main__':
    infer = Inference()
    infer.selected_operation()
