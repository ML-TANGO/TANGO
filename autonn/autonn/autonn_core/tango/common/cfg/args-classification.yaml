# args.yaml for ResNet-Cifar10

# base
weights: '' # checkpoint (pt/file/path)
cfg: '' # model config (yaml/file/path)
data: '' # dataset config (dataset.yaml)
hyp: '' # hyperparameter config (hyp.yaml)
epochs: 500
batch_size: -1  # it'll be changed by auto-batch
img_size: [256, 256]
device: '' # '0' or '0, 1' or 'cpu' (it'll be changed by select-device if '') 

# transfer learning
resume: False # start from checkpoint
freeze: [0] # list of not training layers (transfer learning)

# input images
seed: 1 # random seed for reproducible result
workers: 8 # data loading thread workers
multi_scale: False # randomly scale image per epoch (x0.5 ~ x1.5)
single_cls: False # treat all classes as the one same class
image_weights: False # give sampling priority low accuracy class

# optimizer
adam: False # optimizer = SGD / ADAM
linear_lr: True # schedular = OneCycleLR / LinearLR ref.https://arxiv.org/pdf/1812.01187

# speedy training
cache_images: False # save all images into local memory
nosave: False # not save any checkpoints
notest: False # not measure accuracy during training
patience: '' # early stopping

# hpo
evolve: False # search optimal hyperparameters

# loss/metric
quad: False # loss *= 4
loss_name: 'CE' # loss function = CE(CrossEntropyLoss) / FL(FocalLoss)
label_smoothing: 0.1 # softening hard label ref.https://arxiv.org/abs/1512.00567

# ddp
sync_bn: False
local_rank: -1 # data distributed option (-1 means using only one GPU)

# save
bucket: '' # gsutils (google storage) bucket
project: 'runs/train' # it will be changed to /shared/common/{uid}/{pid}
name: 'autonn' # directory to save files during training (/shared/common/{uid}/{pid}/autonn)
exist_ok: True # overwrite existing directory(T) or make incremental directories(F)
